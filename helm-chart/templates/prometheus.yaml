{{ if .Values.prometheus.useDefault }}

---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    {{- include "kube-hpa-scale-to-zero.labels" . | nindent 4 }}
    app: {{ include "kube-hpa-scale-to-zero.fullname" . }}
    prometheus: {{ .Values.prometheus.selector }}
    release: prom
  name: {{ include "kube-hpa-scale-to-zero.fullname" . }}
spec:
  endpoints:
    - interval: 10s
      path: /metrics
      targetPort: {{ .Values.app.port }}
      port: metrics
  selector:
    matchLabels:
      {{- include "kube-hpa-scale-to-zero.selectorLabels" . | nindent 6 }}
      app: {{ include "kube-hpa-scale-to-zero.fullname" . }}

{{ if .Values.prometheus.alerts.useDefault }}
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: kube-scale-to-zero
  labels:
    {{- include "kube-hpa-scale-to-zero.labels" . | nindent 4 }}
    prometheus: {{ .Values.prometheus.selector }}
spec:
  groups:
    - name: ScaleToZeroPanic
      rules:
        - alert: ScaleToZeroPanic
          expr: |-
            delta(scale_to_zero_panics{service="kube-hpa-scale-to-zero"}[2m])>0
          for: {{ .Values.prometheus.alerts.panics.for }}
          labels:
            {{ if .Values.prometheus.alerts.labels }}
            {{- toYaml .Values.prometheus.alerts.labels | nindent 12 }}
            {{ end }}
            severity: {{ .Values.prometheus.alerts.panics.severity }}
          annotations:
            summary: Scale-to-zero is unable to scale HPA target
            description: {{ .Values.prometheus.alerts.panics.description | toYaml }}

    - name: ScaleToZeroMetricError
      rules:
        - alert: ScaleToZeroControllerUnableToDetermineIfScalingRequired
          expr: |-
            rate(scale_to_zero_errors{service="kube-hpa-scale-to-zero", type="metric"}[1m])
            * on (target_namespace) group_left(slack_channel)
              label_replace(
                  label_replace(
                        kube_namespace_labels,
                        "slack_channel",
                        "$1", "label_spscommerce_com_slack_channel",
                        "(.+)"
                        ),
                  "target_namespace",
                  "$1", "namespace",
                  "(.+)"
              )
            * on (target_namespace) group_left(pagerduty_routing_key)
              label_replace(
                  label_replace(
                      kube_namespace_labels,
                      "pagerduty_routing_key",
                      "$1",
                      "label_spscommerce_com_pagerduty_routing_key",
                      "(.+)"
                  ),
                  "target_namespace",
                  "$1", "namespace",
                  "(.+)"
              )
            >0
          for: {{ .Values.prometheus.alerts.metricErrors.for }}
          labels:
            {{ if .Values.prometheus.alerts.labels }}
            {{- toYaml .Values.prometheus.alerts.labels | nindent 12 }}
            {{ end }}
            severity: {{ .Values.prometheus.alerts.metricErrors.severity }}
          annotations:
            summary: Scale-to-zero is unable to scale HPA target
            description: {{ .Values.prometheus.alerts.metricErrors.description | toYaml }}
    # We route this alert to slack_channel or PD key based on HPA namespace labels
    - name: ScaleToZeroScalingError
      rules:
        - alert: ScaleToZeroControllerUnableToScaleDeployment
          expr: |-
            delta(scale_to_zero_errors{service="kube-hpa-scale-to-zero", type="scaling"}[2m])>0
          for: {{ .Values.prometheus.alerts.scalingErrors.for }}
          labels:
            {{ if .Values.prometheus.alerts.labels }}
            {{- toYaml .Values.prometheus.alerts.labels | nindent 12 }}
            {{ end }}
            severity: {{ .Values.prometheus.alerts.scalingErrors.severity }}
          annotations:
            summary: Scale-to-zero is unable to scale HPA target
            description: {{ .Values.prometheus.alerts.scalingErrors.description | toYaml }}
{{ end }}
{{ end }}